{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1355361,"sourceType":"datasetVersion","datasetId":789090}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-01T09:21:16.071653Z","iopub.execute_input":"2024-09-01T09:21:16.072051Z","iopub.status.idle":"2024-09-01T09:21:16.468762Z","shell.execute_reply.started":"2024-09-01T09:21:16.072013Z","shell.execute_reply":"2024-09-01T09:21:16.467788Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"/kaggle/input/hindi-english-parallel-corpus/hindi_english_parallel.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nimport math\nimport re\nimport string","metadata":{"execution":{"iopub.status.busy":"2024-09-01T09:21:16.471051Z","iopub.execute_input":"2024-09-01T09:21:16.471946Z","iopub.status.idle":"2024-09-01T09:21:19.636067Z","shell.execute_reply.started":"2024-09-01T09:21:16.471895Z","shell.execute_reply":"2024-09-01T09:21:19.635197Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/hindi-english-parallel-corpus/hindi_english_parallel.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-01T09:21:19.637397Z","iopub.execute_input":"2024-09-01T09:21:19.638436Z","iopub.status.idle":"2024-09-01T09:21:26.113479Z","shell.execute_reply.started":"2024-09-01T09:21:19.638386Z","shell.execute_reply":"2024-09-01T09:21:26.112494Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                               hindi  \\\n0    अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें   \n1                    एक्सेर्साइसर पहुंचनीयता अन्वेषक   \n2              निचले पटल के लिए डिफोल्ट प्लग-इन खाका   \n3               ऊपरी पटल के लिए डिफोल्ट प्लग-इन खाका   \n4  उन प्लग-इनों की सूची जिन्हें डिफोल्ट रूप से नि...   \n\n                                          english  \n0  Give your application an accessibility workout  \n1               Accerciser Accessibility Explorer  \n2  The default plugin layout for the bottom panel  \n3     The default plugin layout for the top panel  \n4  A list of plugins that are disabled by default  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>hindi</th>\n      <th>english</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें</td>\n      <td>Give your application an accessibility workout</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>एक्सेर्साइसर पहुंचनीयता अन्वेषक</td>\n      <td>Accerciser Accessibility Explorer</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>निचले पटल के लिए डिफोल्ट प्लग-इन खाका</td>\n      <td>The default plugin layout for the bottom panel</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ऊपरी पटल के लिए डिफोल्ट प्लग-इन खाका</td>\n      <td>The default plugin layout for the top panel</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>उन प्लग-इनों की सूची जिन्हें डिफोल्ट रूप से नि...</td>\n      <td>A list of plugins that are disabled by default</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def preprocess_text(df):\n    \"\"\"\n    Cleans 'english_sent' and 'hindi_sent' columns in the DataFrame.\n    \"\"\"\n    # Lowercase and handle missing values\n    df[\"english\"] = df[\"english\"].fillna(\"\").apply(lambda x: x.lower())\n    df[\"hindi\"] = df[\"hindi\"].fillna(\"\").apply(lambda x: x.lower())\n\n    # Remove URLs\n    url_pattern = r\"http\\S+\"\n    df[\"english\"] = df[\"english\"].apply(lambda x: re.sub(url_pattern, \"\", x))\n    df[\"hindi\"] = df[\"hindi\"].apply(lambda x: re.sub(url_pattern, \"\", x))\n\n    # Remove digits\n    remove_digits = str.maketrans(\"\", \"\",string.digits)\n    df[\"english\"] = df[\"english\"].apply(lambda x : x.translate(remove_digits))\n    df[\"hindi\"] = df[\"hindi\"].apply(lambda x : x.translate(remove_digits))\n    df[\"hindi\"] = df[\"hindi\"].apply(lambda x : re.sub(\"[a-zA-z२३०८१५७९४६]\", \"\", x))\n    # Remove special characters\n    special_characters = set(string.punctuation)\n    df['english'] = df['english'].apply(\n        lambda x: ''.join(ch for ch in x if ch not in special_characters)\n    )\n    df['hindi'] = df['hindi'].apply(\n        lambda x: ''.join(ch for ch in x if ch not in special_characters)\n    )\n\n    # Remove single quotes\n    df['english'] = df['english'].apply(lambda x: re.sub(\"'\", '', x))\n    df['hindi'] = df['hindi'].apply(lambda x: re.sub(\"'\", '', x))\n\n    # Remove extra spaces\n    df['english'] = df['english'].apply(lambda x : x.strip())\n    df['hindi'] = df['hindi'].apply(lambda x : x.strip())\n    df['english'] = df['english'].apply(lambda x : re.sub(\" +\",\" \",x))\n    df['hindi'] = df['hindi'].apply(lambda x : re.sub(\" +\",\" \",x))\n\n\n#     # Add [start] and [end] tags to Hindi sentences\n#     df[\"hindi\"] = df[\"hindi\"].apply(lambda x: \"[start] \" + x + \" [end]\")\n\n    return df\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-01T09:21:26.114674Z","iopub.execute_input":"2024-09-01T09:21:26.115007Z","iopub.status.idle":"2024-09-01T09:21:26.129030Z","shell.execute_reply.started":"2024-09-01T09:21:26.114971Z","shell.execute_reply":"2024-09-01T09:21:26.128084Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"df = preprocess_text(df)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-01T09:21:26.131771Z","iopub.execute_input":"2024-09-01T09:21:26.132115Z","iopub.status.idle":"2024-09-01T09:22:51.374012Z","shell.execute_reply.started":"2024-09-01T09:21:26.132082Z","shell.execute_reply":"2024-09-01T09:22:51.372927Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"                                               hindi  \\\n0    अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें   \n1                    एक्सेर्साइसर पहुंचनीयता अन्वेषक   \n2               निचले पटल के लिए डिफोल्ट प्लगइन खाका   \n3                ऊपरी पटल के लिए डिफोल्ट प्लगइन खाका   \n4  उन प्लगइनों की सूची जिन्हें डिफोल्ट रूप से निष...   \n\n                                          english  \n0  give your application an accessibility workout  \n1               accerciser accessibility explorer  \n2  the default plugin layout for the bottom panel  \n3     the default plugin layout for the top panel  \n4  a list of plugins that are disabled by default  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>hindi</th>\n      <th>english</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें</td>\n      <td>give your application an accessibility workout</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>एक्सेर्साइसर पहुंचनीयता अन्वेषक</td>\n      <td>accerciser accessibility explorer</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>निचले पटल के लिए डिफोल्ट प्लगइन खाका</td>\n      <td>the default plugin layout for the bottom panel</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ऊपरी पटल के लिए डिफोल्ट प्लगइन खाका</td>\n      <td>the default plugin layout for the top panel</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>उन प्लगइनों की सूची जिन्हें डिफोल्ट रूप से निष...</td>\n      <td>a list of plugins that are disabled by default</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def word_count(sentence):\n    return len(sentence.split())\n","metadata":{"execution":{"iopub.status.busy":"2024-09-01T09:22:51.375106Z","iopub.execute_input":"2024-09-01T09:22:51.375417Z","iopub.status.idle":"2024-09-01T09:22:51.380091Z","shell.execute_reply.started":"2024-09-01T09:22:51.375387Z","shell.execute_reply":"2024-09-01T09:22:51.379076Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"df = df[\n    df['english'].apply(lambda x: word_count(x) <= 10) & \n    df['hindi'].apply(lambda x: word_count(x) <= 10)\n]","metadata":{"execution":{"iopub.status.busy":"2024-09-01T09:22:51.381315Z","iopub.execute_input":"2024-09-01T09:22:51.381695Z","iopub.status.idle":"2024-09-01T09:22:55.846900Z","shell.execute_reply.started":"2024-09-01T09:22:51.381651Z","shell.execute_reply":"2024-09-01T09:22:55.846059Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"\n# df = df.iloc[:300000]\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-01T09:22:55.848098Z","iopub.execute_input":"2024-09-01T09:22:55.848431Z","iopub.status.idle":"2024-09-01T09:22:55.854504Z","shell.execute_reply.started":"2024-09-01T09:22:55.848396Z","shell.execute_reply":"2024-09-01T09:22:55.853534Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"(844857, 2)"},"metadata":{}}]},{"cell_type":"code","source":"# df_fl = df[df['hindi'].apply(lambda x: word_count(x) <= 10)]\n# all_unique_words = set()\n# for sentence in df_fl['hindi']:\n#     all_unique_words.update(unique_words(sentence))\n# uwc = len(all_unique_words)\n# print(uwc)","metadata":{"execution":{"iopub.status.busy":"2024-09-01T09:22:55.855699Z","iopub.execute_input":"2024-09-01T09:22:55.855998Z","iopub.status.idle":"2024-09-01T09:22:55.863951Z","shell.execute_reply.started":"2024-09-01T09:22:55.855965Z","shell.execute_reply":"2024-09-01T09:22:55.863071Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\n# Modified Tokenization function to handle special tokens correctly\ndef tokenize_texts(df, src_col, tgt_col):\n    tokenizer_src = tf.keras.preprocessing.text.Tokenizer(filters='')\n    tokenizer_tgt = tf.keras.preprocessing.text.Tokenizer(filters='')\n\n    # Fit the tokenizers on the source and target columns\n    tokenizer_src.fit_on_texts(df[src_col].values)\n    tokenizer_tgt.fit_on_texts(df[tgt_col].values)\n\n    # Get indices for special tokens\n    start_token_idx_src = len(tokenizer_src.word_index) + 1\n    end_token_idx_src = start_token_idx_src + 1\n    start_token_idx_tgt = len(tokenizer_tgt.word_index) + 1\n    end_token_idx_tgt = start_token_idx_tgt + 1\n\n    # Add special tokens to the tokenizer word index\n    tokenizer_src.word_index['<start>'] = start_token_idx_src\n    tokenizer_src.word_index['<end>'] = end_token_idx_src\n    tokenizer_tgt.word_index['<start>'] = start_token_idx_tgt\n    tokenizer_tgt.word_index['<end>'] = end_token_idx_tgt\n\n    # Convert texts to sequences and add <start> and <end> tokens\n    src_sequences = []\n    tgt_sequences = []\n\n    for text in df[src_col].values:\n        seq = tokenizer_src.texts_to_sequences([text])[0]\n        seq = [start_token_idx_src] + seq + [end_token_idx_src]\n        src_sequences.append(seq)\n\n    for text in df[tgt_col].values:\n        seq = tokenizer_tgt.texts_to_sequences([text])[0]\n        seq = [start_token_idx_tgt] + seq + [end_token_idx_tgt]\n        tgt_sequences.append(seq)\n\n    # Pad sequences to have the same length\n    src_sequences = tf.keras.preprocessing.sequence.pad_sequences(src_sequences, padding='post')\n    tgt_sequences = tf.keras.preprocessing.sequence.pad_sequences(tgt_sequences, padding='post')\n\n    return src_sequences, tgt_sequences, tokenizer_src, tokenizer_tgt","metadata":{"execution":{"iopub.status.busy":"2024-09-01T09:22:55.866155Z","iopub.execute_input":"2024-09-01T09:22:55.866544Z","iopub.status.idle":"2024-09-01T09:22:56.254377Z","shell.execute_reply.started":"2024-09-01T09:22:55.866499Z","shell.execute_reply":"2024-09-01T09:22:56.253407Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Assume df is your DataFrame\nsrc_sequences, tgt_sequences, tokenizer_src, tokenizer_tgt = tokenize_texts(df, 'english', 'hindi')\n\n# Define maximum sequence length\nmax_seq_len = max(max(len(seq) for seq in src_sequences), max(len(seq) for seq in tgt_sequences))\n\n# Define vocabulary sizes based on the tokenizer\ninput_vocab_size = len(tokenizer_src.word_index) + 1  # +1 for padding token (if any)\ntarget_vocab_size = len(tokenizer_tgt.word_index) + 1  # +1 for padding token (if any)\n\n# Split the data into train, validation, and test sets\ntrain_src, test_src, train_tgt, test_tgt = train_test_split(src_sequences, tgt_sequences, test_size=0.2, random_state=42)\ntrain_src, val_src, train_tgt, val_tgt = train_test_split(train_src, train_tgt, test_size=0.2, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-01T09:22:56.255595Z","iopub.execute_input":"2024-09-01T09:22:56.256162Z","iopub.status.idle":"2024-09-01T09:23:32.048222Z","shell.execute_reply.started":"2024-09-01T09:22:56.256127Z","shell.execute_reply":"2024-09-01T09:23:32.047407Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Positional Encoding\ndef positional_encoding(position, d_model):\n    angle_rads = np.arange(position)[:, np.newaxis] / np.power(10000, (2 * (np.arange(d_model) // 2)) / np.float32(d_model))\n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n    \n    pos_enc = tf.cast(angle_rads[np.newaxis, ...], dtype=tf.float32)\n    return tf.Variable(pos_enc, trainable=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-01T09:23:32.049410Z","iopub.execute_input":"2024-09-01T09:23:32.049737Z","iopub.status.idle":"2024-09-01T09:23:32.056520Z","shell.execute_reply.started":"2024-09-01T09:23:32.049703Z","shell.execute_reply":"2024-09-01T09:23:32.055404Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Scaled Dot-Product Attention\ndef scaled_dot_product_attention(q, k, v, mask):\n    matmul_qk = tf.matmul(q, k, transpose_b=True)\n    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n    \n    if mask is not None:\n        scaled_attention_logits += (mask * -1e9)  # Large negative value to mask out softmax\n\n    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n    output = tf.matmul(attention_weights, v)\n    \n    return output, attention_weights","metadata":{"execution":{"iopub.status.busy":"2024-09-01T09:23:32.057760Z","iopub.execute_input":"2024-09-01T09:23:32.058124Z","iopub.status.idle":"2024-09-01T09:23:32.067945Z","shell.execute_reply.started":"2024-09-01T09:23:32.058082Z","shell.execute_reply":"2024-09-01T09:23:32.067011Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Multi-Head Attention Layer\nclass MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        self.num_heads = num_heads\n        self.d_model = d_model\n        self.depth = d_model // num_heads\n\n        self.wq = tf.keras.layers.Dense(d_model)\n        self.wk = tf.keras.layers.Dense(d_model)\n        self.wv = tf.keras.layers.Dense(d_model)\n        self.dense = tf.keras.layers.Dense(d_model)\n\n    def split_heads(self, x, batch_size):\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, q, k, v, mask):\n        batch_size = tf.shape(q)[0]\n        q = self.split_heads(self.wq(q), batch_size)\n        k = self.split_heads(self.wk(k), batch_size)\n        v = self.split_heads(self.wv(v), batch_size)\n\n        output, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n        output = tf.transpose(output, perm=[0, 2, 1, 3])\n        output = tf.reshape(output, (batch_size, -1, self.d_model))\n\n        return self.dense(output), attention_weights\n","metadata":{"execution":{"iopub.status.busy":"2024-09-01T09:23:32.072251Z","iopub.execute_input":"2024-09-01T09:23:32.073083Z","iopub.status.idle":"2024-09-01T09:23:32.082928Z","shell.execute_reply.started":"2024-09-01T09:23:32.073028Z","shell.execute_reply":"2024-09-01T09:23:32.081910Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Feed Forward Network Layer\nclass FeedForwardNetwork(tf.keras.layers.Layer):\n    def __init__(self, d_model, dff):\n        super(FeedForwardNetwork, self).__init__()\n        self.dense1 = tf.keras.layers.Dense(dff, activation='relu')\n        self.dense2 = tf.keras.layers.Dense(d_model)\n\n    def call(self, x):\n        x = self.dense1(x)\n        return self.dense2(x)","metadata":{"execution":{"iopub.status.busy":"2024-09-01T09:23:32.084057Z","iopub.execute_input":"2024-09-01T09:23:32.084405Z","iopub.status.idle":"2024-09-01T09:23:32.095737Z","shell.execute_reply.started":"2024-09-01T09:23:32.084326Z","shell.execute_reply":"2024-09-01T09:23:32.094958Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Encoder Layer\nclass EncoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.3):\n        super(EncoderLayer, self).__init__()\n        self.mha = MultiHeadAttention(d_model, num_heads)\n        self.ffn = FeedForwardNetwork(d_model, dff)\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        \n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, training, mask=None):\n        attn_output, _ = self.mha(x, x, x, mask)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(x + attn_output)\n\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        out2 = self.layernorm2(out1 + ffn_output)\n\n        return out2","metadata":{"execution":{"iopub.status.busy":"2024-09-01T09:23:32.097011Z","iopub.execute_input":"2024-09-01T09:23:32.097389Z","iopub.status.idle":"2024-09-01T09:23:32.106325Z","shell.execute_reply.started":"2024-09-01T09:23:32.097331Z","shell.execute_reply":"2024-09-01T09:23:32.105335Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"class Encoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, dff, vocab_size, pe_input, rate=0.3):\n        super(Encoder, self).__init__()\n        self.num_layers = num_layers\n        self.d_model = d_model\n        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n        self.pos_encoding = positional_encoding(pe_input, self.d_model)\n        \n        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n        self.dropout = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, training, mask=None):\n        seq_len = tf.shape(x)[1]\n        x = self.embedding(x)\n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x += self.pos_encoding[:, :seq_len, :]\n        x = self.dropout(x, training=training)\n\n        for i in range(self.num_layers):\n            x = self.enc_layers[i](x, training=training, mask=mask)\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-09-01T09:23:32.107540Z","iopub.execute_input":"2024-09-01T09:23:32.108554Z","iopub.status.idle":"2024-09-01T09:23:32.118770Z","shell.execute_reply.started":"2024-09-01T09:23:32.108514Z","shell.execute_reply":"2024-09-01T09:23:32.117781Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Decoder Layer\nclass DecoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.3):\n        super(DecoderLayer, self).__init__()\n        self.mha1 = MultiHeadAttention(d_model, num_heads)\n        self.mha2 = MultiHeadAttention(d_model, num_heads)\n        self.ffn = FeedForwardNetwork(d_model, dff)\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        \n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n        self.dropout3 = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, enc_output, training, look_ahead_mask=None, dec_padding_mask=None):\n        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n        attn1 = self.dropout1(attn1, training=training)\n        out1 = self.layernorm1(x + attn1)\n\n        attn2, attn_weights_block2 = self.mha2(out1, enc_output, enc_output, dec_padding_mask)\n        attn2 = self.dropout2(attn2, training=training)\n        out2 = self.layernorm2(out1 + attn2)\n\n        ffn_output = self.ffn(out2)\n        ffn_output = self.dropout3(ffn_output, training=training)\n        out3 = self.layernorm3(out2 + ffn_output)\n\n        return out3, attn_weights_block1, attn_weights_block2","metadata":{"execution":{"iopub.status.busy":"2024-09-01T09:23:32.119916Z","iopub.execute_input":"2024-09-01T09:23:32.120239Z","iopub.status.idle":"2024-09-01T09:23:32.131471Z","shell.execute_reply.started":"2024-09-01T09:23:32.120207Z","shell.execute_reply":"2024-09-01T09:23:32.130418Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"class Decoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate=0.3):\n        super(Decoder, self).__init__()\n        self.num_layers = num_layers\n        self.d_model = d_model\n        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n        self.pos_encoding = positional_encoding(pe_target, d_model)\n        \n        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n        self.dropout = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, enc_output, training, look_ahead_mask=None, dec_padding_mask=None):\n        attention_weights = {}\n        \n        seq_len = tf.shape(x)[1]\n        x = self.embedding(x)\n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x += self.pos_encoding[:, :seq_len, :]\n        x = self.dropout(x, training=training)\n\n        for i in range(self.num_layers):\n            x, block1, block2 = self.dec_layers[i](x, enc_output, training=training, look_ahead_mask=look_ahead_mask, dec_padding_mask=dec_padding_mask)\n            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n\n        return x, attention_weights","metadata":{"execution":{"iopub.status.busy":"2024-09-01T09:23:32.132691Z","iopub.execute_input":"2024-09-01T09:23:32.133094Z","iopub.status.idle":"2024-09-01T09:23:32.145197Z","shell.execute_reply.started":"2024-09-01T09:23:32.133054Z","shell.execute_reply":"2024-09-01T09:23:32.144407Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"class Transformer(tf.keras.Model):\n    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.3):\n        super(Transformer, self).__init__()\n        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n\n    def call(self, inp, tar, training=None, enc_padding_mask=None, look_ahead_mask=None, dec_padding_mask=None):\n        enc_output = self.encoder(inp, training=training, mask=enc_padding_mask)\n        dec_output, attention_weights = self.decoder(\n            tar, enc_output, training=training, look_ahead_mask=look_ahead_mask, dec_padding_mask=dec_padding_mask\n        )\n        final_output = self.final_layer(dec_output)\n\n        return final_output, attention_weights","metadata":{"execution":{"iopub.status.busy":"2024-09-01T09:23:32.146270Z","iopub.execute_input":"2024-09-01T09:23:32.146599Z","iopub.status.idle":"2024-09-01T09:23:32.159359Z","shell.execute_reply.started":"2024-09-01T09:23:32.146567Z","shell.execute_reply":"2024-09-01T09:23:32.158574Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# Instantiate the Transformer model\nd_model = 512\ndff = 2048\nnum_layers = 6\nnum_heads = 8\npe_input = max_seq_len\npe_target = max_seq_len\n","metadata":{"execution":{"iopub.status.busy":"2024-09-01T09:23:32.160411Z","iopub.execute_input":"2024-09-01T09:23:32.160693Z","iopub.status.idle":"2024-09-01T09:23:32.172476Z","shell.execute_reply.started":"2024-09-01T09:23:32.160661Z","shell.execute_reply":"2024-09-01T09:23:32.171655Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# Helper functions for masks\ndef create_padding_mask(seq):\n    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n    return seq[:, tf.newaxis, tf.newaxis, :]\n\ndef create_look_ahead_mask(size):\n    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n    return mask\n\ndef create_masks(inp, tar):\n    enc_padding_mask = create_padding_mask(inp)\n    dec_padding_mask = create_padding_mask(inp)\n    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n    dec_target_padding_mask = create_padding_mask(tar)\n    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n    \n    return enc_padding_mask, combined_mask, dec_padding_mask\n","metadata":{"execution":{"iopub.status.busy":"2024-09-01T09:23:32.173765Z","iopub.execute_input":"2024-09-01T09:23:32.174122Z","iopub.status.idle":"2024-09-01T09:23:32.183040Z","shell.execute_reply.started":"2024-09-01T09:23:32.174080Z","shell.execute_reply":"2024-09-01T09:23:32.182127Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"\n# Loss function\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n\ndef loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)","metadata":{"execution":{"iopub.status.busy":"2024-09-01T09:23:32.184166Z","iopub.execute_input":"2024-09-01T09:23:32.185636Z","iopub.status.idle":"2024-09-01T09:23:32.192536Z","shell.execute_reply.started":"2024-09-01T09:23:32.185602Z","shell.execute_reply":"2024-09-01T09:23:32.191678Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Learning rate schedule\nclass CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, d_model, warmup_steps=4000):\n        super(CustomSchedule, self).__init__()\n        self.d_model = d_model\n        self.d_model = tf.cast(self.d_model, tf.float32)\n        self.warmup_steps = warmup_steps\n\n    def __call__(self, step):\n        step = tf.cast(step, tf.float32)\n        arg1 = tf.math.rsqrt(step)\n        arg2 = step * (self.warmup_steps ** -1.5)\n        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n\n# Metrics\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\nval_loss = tf.keras.metrics.Mean(name='val_loss')\nval_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')\ntest_loss = tf.keras.metrics.Mean(name='test_loss')\ntest_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')","metadata":{"execution":{"iopub.status.busy":"2024-09-01T09:23:32.193659Z","iopub.execute_input":"2024-09-01T09:23:32.193954Z","iopub.status.idle":"2024-09-01T09:23:32.685967Z","shell.execute_reply.started":"2024-09-01T09:23:32.193923Z","shell.execute_reply":"2024-09-01T09:23:32.684965Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"\n# Training and validation step functions\n@tf.function\ndef train_step(inp, tar, transformer, optimizer, loss_function, train_loss, train_accuracy):\n    tar_inp = tar[:, :-1]\n    tar_real = tar[:, 1:]\n\n    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n\n    with tf.GradientTape() as tape:\n        predictions, _ = transformer(\n            inp, tar_inp, training=True,\n            enc_padding_mask=enc_padding_mask, \n            look_ahead_mask=combined_mask, \n            dec_padding_mask=dec_padding_mask\n        )\n        loss = loss_function(tar_real, predictions)\n\n    gradients = tape.gradient(loss, transformer.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n\n    train_loss.update_state(loss)\n    train_accuracy.update_state(tar_real, predictions)\n\n@tf.function\ndef val_step(inp, tar, transformer, loss_function, val_loss, val_accuracy):\n    tar_inp = tar[:, :-1]\n    tar_real = tar[:, 1:]\n\n    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n\n    predictions, _ = transformer(\n        inp, tar_inp, training=False,\n        enc_padding_mask=enc_padding_mask, \n        look_ahead_mask=combined_mask, \n        dec_padding_mask=dec_padding_mask\n    )\n    loss = loss_function(tar_real, predictions)\n\n    val_loss.update_state(loss)\n    val_accuracy.update_state(tar_real, predictions)","metadata":{"execution":{"iopub.status.busy":"2024-09-01T09:23:32.687086Z","iopub.execute_input":"2024-09-01T09:23:32.687411Z","iopub.status.idle":"2024-09-01T09:23:32.698400Z","shell.execute_reply.started":"2024-09-01T09:23:32.687378Z","shell.execute_reply":"2024-09-01T09:23:32.697562Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef val_step(inp, tar, transformer, loss_function, val_loss, val_accuracy):\n    tar_inp = tar[:, :-1]\n    tar_real = tar[:, 1:]\n\n    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n\n    predictions, _ = transformer(\n        inp, tar_inp, training=False,\n        enc_padding_mask=enc_padding_mask, \n        look_ahead_mask=combined_mask, \n        dec_padding_mask=dec_padding_mask\n    )\n    loss = loss_function(tar_real, predictions)\n\n    val_loss.update_state(loss)\n    val_accuracy.update_state(tar_real, predictions)","metadata":{"execution":{"iopub.status.busy":"2024-09-01T09:23:32.699405Z","iopub.execute_input":"2024-09-01T09:23:32.699674Z","iopub.status.idle":"2024-09-01T09:23:32.711778Z","shell.execute_reply.started":"2024-09-01T09:23:32.699645Z","shell.execute_reply":"2024-09-01T09:23:32.711038Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# Test step function\n@tf.function\ndef test_step(inp, tar, transformer, loss_function, test_loss, test_accuracy):\n    tar_inp = tar[:, :-1]\n    tar_real = tar[:, 1:]\n\n    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n\n    predictions, _ = transformer(\n        inp, tar_inp, training=False,\n        enc_padding_mask=enc_padding_mask, \n        look_ahead_mask=combined_mask, \n        dec_padding_mask=dec_padding_mask\n    )\n    loss = loss_function(tar_real, predictions)\n\n    test_loss.update_state(loss)\n    test_accuracy.update_state(tar_real, predictions)","metadata":{"execution":{"iopub.status.busy":"2024-09-01T09:23:32.712737Z","iopub.execute_input":"2024-09-01T09:23:32.713025Z","iopub.status.idle":"2024-09-01T09:23:32.721674Z","shell.execute_reply.started":"2024-09-01T09:23:32.712994Z","shell.execute_reply":"2024-09-01T09:23:32.720886Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"BUFFER_SIZE = 50000\nBATCH_SIZE = 128\n\n\n# Data preparation for training, validation, and testing\ntrain_dataset = tf.data.Dataset.from_tensor_slices((train_src, train_tgt))\ntrain_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n\nval_dataset = tf.data.Dataset.from_tensor_slices((val_src, val_tgt))\nval_dataset = val_dataset.batch(BATCH_SIZE)\n\ntest_dataset = tf.data.Dataset.from_tensor_slices((test_src, test_tgt))\ntest_dataset = test_dataset.batch(BATCH_SIZE)\n\n# Instantiate the transformer model\nnum_layers = 4\nd_model = 128\nnum_heads = 8\ndff = 512\npe_input = max_seq_len\npe_target = max_seq_len\n\ntransformer = Transformer(num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target)\n\n# Define the learning rate schedule and optimizer\nlearning_rate = CustomSchedule(d_model)\noptimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n\n# Early Stopping Configuration\nbest_val_loss = float('inf')\npatience_counter = 0","metadata":{"execution":{"iopub.status.busy":"2024-09-01T09:23:32.722800Z","iopub.execute_input":"2024-09-01T09:23:32.723115Z","iopub.status.idle":"2024-09-01T09:23:33.007002Z","shell.execute_reply.started":"2024-09-01T09:23:32.723069Z","shell.execute_reply":"2024-09-01T09:23:33.006188Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 10\n\n# Lists to track metrics over epochs for plotting\ntrain_loss_history = []\nval_loss_history = []\ntest_loss_history = []\ntrain_accuracy_history = []\nval_accuracy_history = []\ntest_accuracy_history = []\n\n# Training loop with validation and early stopping\nfor epoch in range(EPOCHS):\n    train_loss.reset_state()  # Correct method to reset the metric\n    train_accuracy.reset_state()  # Correct method to reset the metric\n    val_loss.reset_state()  # Correct method to reset the metric\n    val_accuracy.reset_state()  # Correct method to reset the metric\n\n    for (batch, (inp, tar)) in enumerate(train_dataset):\n        train_step(inp, tar, transformer, optimizer, loss_function, train_loss, train_accuracy)\n        print(f'Epoch {epoch + 1} Train Loss {train_loss.result():.4f} Train Accuracy {train_accuracy.result():.4f}')\n\n    for (batch, (inp, tar)) in enumerate(val_dataset):\n        val_step(inp, tar, transformer, loss_function, val_loss, val_accuracy)\n\n    print(f'Epoch {epoch + 1} Train Loss {train_loss.result():.4f} Train Accuracy {train_accuracy.result():.4f}')\n    print(f'Epoch {epoch + 1} Validation Loss {val_loss.result():.4f} Validation Accuracy {val_accuracy.result():.4f}')\n\n    # Save metrics for plotting\n    train_loss_history.append(train_loss.result().numpy())\n    val_loss_history.append(val_loss.result().numpy())\n    train_accuracy_history.append(train_accuracy.result().numpy())\n    val_accuracy_history.append(val_accuracy.result().numpy())\n\n\n# Testing the Model after Training\ntest_loss.reset_state()  # Correct method to reset the metric\ntest_accuracy.reset_state()  # Correct method to reset the metric\n\nfor (batch, (inp, tar)) in enumerate(test_dataset):\n    test_step(inp, tar, transformer, loss_function, test_loss, test_accuracy)\n\nprint(f'Test Loss: {test_loss.result():.4f}')\nprint(f'Test Accuracy: {test_accuracy.result():.4f}')\n\n# Save test metrics for plotting\ntest_loss_history.append(test_loss.result().numpy())\ntest_accuracy_history.append(test_accuracy.result().numpy())\n","metadata":{"execution":{"iopub.status.busy":"2024-09-01T09:26:20.583834Z","iopub.execute_input":"2024-09-01T09:26:20.584597Z","iopub.status.idle":"2024-09-01T09:26:20.590276Z","shell.execute_reply.started":"2024-09-01T09:26:20.584554Z","shell.execute_reply":"2024-09-01T09:26:20.589278Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Plotting Training, Validation, and Test Loss\nplt.figure(figsize=(10, 6))\nplt.plot(train_loss_history, label='Training Loss')\nplt.plot(val_loss_history, label='Validation Loss')\nplt.plot([len(train_loss_history)-1]*len(test_loss_history), test_loss_history, label='Test Loss', linestyle='dashed')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training, Validation, and Test Loss')\nplt.legend()\nplt.show()\n\n# Plotting Training, Validation, and Test Accuracy\nplt.figure(figsize=(10, 6))\nplt.plot(train_accuracy_history, label='Training Accuracy')\nplt.plot(val_accuracy_history, label='Validation Accuracy')\nplt.plot([len(train_accuracy_history)-1]*len(test_accuracy_history), test_accuracy_history, label='Test Accuracy', linestyle='dashed')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Training, Validation, and Test Accuracy')\nplt.legend()\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Inference function with improved handling of special tokens and sequence generation\ndef evaluate(inp_sentence, max_output_length=10):\n    start_token = [tokenizer_src.word_index['<start>']]\n    end_token = [tokenizer_src.word_index['<end>']]\n\n    # Convert input sentence to a sequence of tokens, adding <start> and <end> tokens\n    inp_sequence = start_token + tokenizer_src.texts_to_sequences([inp_sentence])[0] + end_token\n    inp_sequence = tf.keras.preprocessing.sequence.pad_sequences([inp_sequence], maxlen=max_seq_len, padding='post')\n\n    # Prepare the encoder input and initial decoder output (<start> token)\n    encoder_input = tf.convert_to_tensor(inp_sequence)\n    output = tf.convert_to_tensor([start_token])\n\n    for i in range(max_output_length):\n        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n\n        # Run the Transformer to get predictions\n        predictions, _ = transformer(\n            encoder_input, output, training=False,\n            enc_padding_mask=enc_padding_mask, look_ahead_mask=combined_mask, dec_padding_mask=dec_padding_mask\n        )\n\n        # Select the last time step's prediction\n        predictions = predictions[:, -1:, :]\n        \n        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n\n        # Check if the predicted ID is the <end> token\n        if predicted_id == end_token[0]:\n            return tf.squeeze(output, axis=0)\n\n        # Concatenate the predicted ID to the output sequence\n        output = tf.concat([output, predicted_id], axis=-1)\n\n    return tf.squeeze(output, axis=0)\n\n# Translation function\ndef translate(sentence):\n    # Evaluate the sentence and convert result to human-readable form\n    result = evaluate(sentence)\n    # Extract non-padding, non-special tokens from the output\n    predicted_sentence = tokenizer_tgt.sequences_to_texts([[i for i in result.numpy() if i > 0 and i != tokenizer_tgt.word_index['<start>'] and i != tokenizer_tgt.word_index['<end>']]])[0]\n    print(f'Input: {sentence}')\n    print(f'Predicted translation: {predicted_sentence}')\n\n# Example usage\ntranslate(\"pridicting is not good thing which should be promoted\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}